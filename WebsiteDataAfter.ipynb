{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from string import digits\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the (new) data from the Scraper for after the date effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Scraper/AfterProcessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups_aft = dict()\n",
    "companies_aft = os.listdir()\n",
    "companies_aft = companies_aft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the files of the scraped websites and create a BeautifulSoup object for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "  1%|▊                                                                                 | 1/100 [00:08<13:36,  8.25s/it]\n",
      "  3%|██▍                                                                               | 3/100 [00:10<09:48,  6.07s/it]\n",
      "  6%|████▉                                                                             | 6/100 [00:10<06:42,  4.28s/it]\n",
      "  8%|██████▌                                                                           | 8/100 [00:10<04:41,  3.05s/it]\n",
      " 10%|████████                                                                         | 10/100 [00:12<03:37,  2.41s/it]\n",
      " 11%|████████▉                                                                        | 11/100 [00:13<02:40,  1.81s/it]\n",
      " 12%|█████████▋                                                                       | 12/100 [00:13<02:05,  1.43s/it]\n",
      " 14%|███████████▎                                                                     | 14/100 [00:15<01:54,  1.33s/it]\n",
      " 15%|████████████▏                                                                    | 15/100 [00:17<02:09,  1.52s/it]\n",
      " 16%|████████████▉                                                                    | 16/100 [00:18<01:48,  1.30s/it]\n",
      " 17%|█████████████▊                                                                   | 17/100 [00:19<01:24,  1.02s/it]\n",
      " 19%|███████████████▍                                                                 | 19/100 [00:19<01:04,  1.26it/s]\n",
      " 20%|████████████████▏                                                                | 20/100 [00:20<01:04,  1.25it/s]\n",
      " 21%|█████████████████                                                                | 21/100 [00:21<01:05,  1.20it/s]\n",
      " 22%|█████████████████▊                                                               | 22/100 [00:22<01:15,  1.04it/s]\n",
      " 23%|██████████████████▋                                                              | 23/100 [00:24<01:30,  1.18s/it]\n",
      " 24%|███████████████████▍                                                             | 24/100 [00:25<01:38,  1.29s/it]\n",
      " 25%|████████████████████▎                                                            | 25/100 [00:31<03:23,  2.72s/it]\n",
      " 26%|█████████████████████                                                            | 26/100 [00:33<03:03,  2.49s/it]\n",
      " 27%|█████████████████████▊                                                           | 27/100 [00:44<06:05,  5.01s/it]\n",
      " 28%|██████████████████████▋                                                          | 28/100 [00:53<07:14,  6.03s/it]\n",
      " 29%|███████████████████████▍                                                         | 29/100 [01:05<09:18,  7.87s/it]\n",
      " 30%|████████████████████████▎                                                        | 30/100 [01:05<06:35,  5.65s/it]\n",
      " 32%|█████████████████████████▉                                                       | 32/100 [01:05<04:31,  4.00s/it]\n",
      " 33%|██████████████████████████▋                                                      | 33/100 [01:06<03:09,  2.83s/it]\n",
      " 34%|███████████████████████████▌                                                     | 34/100 [01:07<02:30,  2.28s/it]\n",
      " 35%|████████████████████████████▎                                                    | 35/100 [01:15<04:32,  4.20s/it]\n",
      " 36%|█████████████████████████████▏                                                   | 36/100 [01:16<03:21,  3.15s/it]\n",
      " 37%|█████████████████████████████▉                                                   | 37/100 [01:21<03:47,  3.61s/it]\n",
      " 38%|██████████████████████████████▊                                                  | 38/100 [01:26<04:15,  4.13s/it]\n",
      " 39%|███████████████████████████████▌                                                 | 39/100 [01:27<03:17,  3.24s/it]\n",
      " 40%|████████████████████████████████▍                                                | 40/100 [01:31<03:21,  3.36s/it]\n",
      " 41%|█████████████████████████████████▏                                               | 41/100 [01:38<04:22,  4.46s/it]\n",
      " 42%|██████████████████████████████████                                               | 42/100 [01:38<03:05,  3.20s/it]\n",
      " 44%|███████████████████████████████████▋                                             | 44/100 [01:38<02:07,  2.29s/it]\n",
      " 45%|████████████████████████████████████▍                                            | 45/100 [01:42<02:28,  2.70s/it]\n",
      " 46%|█████████████████████████████████████▎                                           | 46/100 [01:42<01:45,  1.96s/it]\n",
      " 47%|██████████████████████████████████████                                           | 47/100 [01:43<01:22,  1.55s/it]\n",
      " 48%|██████████████████████████████████████▉                                          | 48/100 [01:43<01:03,  1.22s/it]\n",
      " 49%|███████████████████████████████████████▋                                         | 49/100 [01:44<00:49,  1.04it/s]\n",
      " 50%|████████████████████████████████████████▌                                        | 50/100 [01:44<00:35,  1.40it/s]\n",
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [01:47<01:14,  1.52s/it]\n",
      " 56%|█████████████████████████████████████████████▎                                   | 56/100 [01:53<01:03,  1.44s/it]\n",
      " 57%|██████████████████████████████████████████████▏                                  | 57/100 [01:58<01:42,  2.38s/it]\n",
      " 58%|██████████████████████████████████████████████▉                                  | 58/100 [01:59<01:20,  1.91s/it]\n",
      " 59%|███████████████████████████████████████████████▊                                 | 59/100 [02:00<01:06,  1.63s/it]\n",
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [02:01<00:59,  1.49s/it]\n",
      " 62%|██████████████████████████████████████████████████▏                              | 62/100 [02:01<00:40,  1.08s/it]\n",
      " 63%|███████████████████████████████████████████████████                              | 63/100 [02:02<00:33,  1.12it/s]\n",
      " 65%|████████████████████████████████████████████████████▋                            | 65/100 [02:03<00:30,  1.15it/s]\n",
      " 67%|██████████████████████████████████████████████████████▎                          | 67/100 [02:06<00:33,  1.02s/it]\n",
      " 68%|███████████████████████████████████████████████████████                          | 68/100 [02:08<00:38,  1.19s/it]\n",
      " 69%|███████████████████████████████████████████████████████▉                         | 69/100 [02:10<00:49,  1.61s/it]\n",
      " 71%|█████████████████████████████████████████████████████████▌                       | 71/100 [02:12<00:39,  1.37s/it]\n",
      " 73%|███████████████████████████████████████████████████████████▏                     | 73/100 [02:13<00:31,  1.16s/it]\n",
      " 81%|█████████████████████████████████████████████████████████████████▌               | 81/100 [02:14<00:15,  1.20it/s]\n",
      " 82%|██████████████████████████████████████████████████████████████████▍              | 82/100 [02:14<00:11,  1.51it/s]\n",
      " 84%|████████████████████████████████████████████████████████████████████             | 84/100 [02:17<00:14,  1.07it/s]\n",
      " 85%|████████████████████████████████████████████████████████████████████▊            | 85/100 [02:19<00:17,  1.15s/it]\n",
      " 86%|█████████████████████████████████████████████████████████████████████▋           | 86/100 [02:19<00:12,  1.09it/s]\n",
      " 88%|███████████████████████████████████████████████████████████████████████▎         | 88/100 [02:25<00:18,  1.52s/it]\n",
      " 89%|████████████████████████████████████████████████████████████████████████         | 89/100 [02:31<00:30,  2.80s/it]\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▋       | 91/100 [02:31<00:17,  1.98s/it]\n",
      " 92%|██████████████████████████████████████████████████████████████████████████▌      | 92/100 [02:37<00:24,  3.10s/it]\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▎     | 93/100 [02:37<00:16,  2.36s/it]\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▏    | 94/100 [02:40<00:14,  2.48s/it]\n",
      " 96%|█████████████████████████████████████████████████████████████████████████████▊   | 96/100 [02:40<00:07,  1.75s/it]\n",
      " 97%|██████████████████████████████████████████████████████████████████████████████▌  | 97/100 [02:41<00:04,  1.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:42<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(companies_aft):\n",
    "    with open(file, encoding='utf-8') as data:\n",
    "        soup = BeautifulSoup(data, 'html.parser',from_encoding='utf-8')\n",
    "        soups_aft.update({file[:-4]:soup})\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the items for every website to a dictionary. Only process items that do not have a 'forbidden string' in their URL. Remove all style, script and metadata from the html. Only if the text stored between the <html> tags is recognized as HTML, the item is added to the dataset. Also remove the metadata from Wayback Machine by removing the banners and the text between \"success fail\" and \"TIMESTAMPS\". Remove all words that have a . in the middle of the word and remove all numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800contacts20140620\n",
      "333\n",
      "207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▊                                                                              | 1/100 [04:23<7:14:41, 263.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1hsa20120419\n",
      "3\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|█▌                                                                             | 2/100 [04:25<5:02:09, 184.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa20140531\n",
      "131\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|██▎                                                                            | 3/100 [07:27<4:57:44, 184.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa20150724\n",
      "5\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|███▏                                                                           | 4/100 [07:33<3:28:54, 130.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abaseguros20150502\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|████                                                                            | 5/100 [07:34<2:25:24, 91.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcassicura20120730\n",
      "92\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|████▊                                                                           | 6/100 [08:17<2:00:50, 77.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdainsurance20190613\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|█████▌                                                                          | 7/100 [08:20<1:24:57, 54.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aberdare20160317\n",
      "77\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▍                                                                         | 8/100 [09:19<1:26:12, 56.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abrihealthplan20120901\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|███████▏                                                                        | 9/100 [09:20<1:00:03, 39.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolutetotalcare20120601\n",
      "270\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|███████▉                                                                       | 10/100 [12:04<1:55:16, 76.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acariahealth20150401\n",
      "68\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|████████▋                                                                      | 11/100 [12:44<1:37:40, 65.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessintegra20170902\n",
      "110\n",
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█████████▍                                                                     | 12/100 [13:36<1:30:33, 61.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessplansusa20140531\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|██████████▎                                                                    | 13/100 [13:37<1:03:11, 43.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accredited-inc20161127\n",
      "308\n",
      "282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|███████████                                                                    | 14/100 [17:08<2:14:32, 93.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acibademsigorta20151113\n",
      "179\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|███████████▋                                                                  | 15/100 [19:41<2:37:46, 111.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acibademsigorta20181005\n",
      "55\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|████████████▋                                                                  | 16/100 [20:32<2:10:38, 93.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acrcapitalholdings20130208\n",
      "13\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▍                                                                 | 17/100 [20:59<1:41:36, 73.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acrcapitalholdings20140515\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|██████████████▏                                                                | 18/100 [20:59<1:10:23, 51.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantagedental20180108\n",
      "46\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|███████████████                                                                | 19/100 [21:46<1:07:31, 50.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20130103\n",
      "66\n",
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|███████████████▊                                                               | 20/100 [23:33<1:29:40, 67.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20130628\n",
      "68\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|████████████████▌                                                              | 21/100 [25:10<1:40:13, 76.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20130930\n",
      "85\n",
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|█████████████████▍                                                             | 22/100 [27:18<1:59:09, 91.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20131001\n",
      "101\n",
      "98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|█████████████████▉                                                            | 23/100 [30:17<2:31:23, 117.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20131005\n",
      "88\n",
      "85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██████████████████▋                                                           | 24/100 [32:48<2:41:41, 127.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20150401\n",
      "327\n",
      "303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|███████████████████▌                                                          | 25/100 [40:43<4:49:56, 231.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20150507\n",
      "84\n",
      "78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|████████████████████▎                                                         | 26/100 [42:40<4:03:34, 197.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20160423\n",
      "414\n",
      "352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|█████████████████████                                                         | 27/100 [52:01<6:13:02, 306.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20161125\n",
      "314\n",
      "222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|█████████████████████▊                                                        | 28/100 [59:30<6:58:56, 349.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20190725\n",
      "500\n",
      "402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██████████████████████                                                      | 29/100 [1:08:53<8:09:16, 413.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afaforsakring20180401\n",
      "58\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|██████████████████████▊                                                     | 30/100 [1:09:33<5:51:31, 301.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affinityinsuranceservices20140531\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███████████████████████▌                                                    | 31/100 [1:09:34<4:02:58, 211.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agenzie20181201\n",
      "36\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|████████████████████████▎                                                   | 32/100 [1:10:12<3:00:25, 159.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmchealth20131118\n",
      "21\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|█████████████████████████                                                   | 33/100 [1:10:23<2:08:12, 114.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmpr20130208\n",
      "93\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|█████████████████████████▊                                                  | 34/100 [1:11:59<2:00:07, 109.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahn20150429\n",
      "500\n",
      "447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|██████████████████████████▌                                                 | 35/100 [1:29:42<7:08:08, 395.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahni20190522\n",
      "107\n",
      "101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███████████████████████████▎                                                | 36/100 [1:31:22<5:27:06, 306.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aia20130105\n",
      "330\n",
      "154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|████████████████████████████                                                | 37/100 [1:36:27<5:21:42, 306.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aig20190510\n",
      "436\n",
      "379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|████████████████████████████▉                                               | 38/100 [1:44:48<6:16:43, 364.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline20140320\n",
      "135\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|█████████████████████████████▋                                              | 39/100 [1:47:50<5:15:03, 309.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajg20171208\n",
      "257\n",
      "249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|██████████████████████████████▍                                             | 40/100 [1:53:14<5:14:02, 314.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajg20181221\n",
      "500\n",
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|███████████████████████████████▏                                            | 41/100 [2:03:37<6:39:54, 406.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alamoinsurance20120105\n",
      "40\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|███████████████████████████████▉                                            | 42/100 [2:04:10<4:44:56, 294.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alan20181025\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████████████████████████████████▋                                           | 43/100 [2:04:11<3:16:20, 206.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alchealth20190302\n",
      "46\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|█████████████████████████████████▍                                          | 44/100 [2:04:59<2:28:17, 158.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegeus20150424\n",
      "317\n",
      "279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|██████████████████████████████████▏                                         | 45/100 [2:10:21<3:10:42, 208.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20130718\n",
      "30\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|██████████████████████████████████▉                                         | 46/100 [2:10:46<2:17:36, 152.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20140501\n",
      "65\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|███████████████████████████████████▋                                        | 47/100 [2:11:39<1:48:46, 123.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20151004\n",
      "63\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████████████████████████████████████▍                                       | 48/100 [2:12:31<1:28:09, 101.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20151212\n",
      "50\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|█████████████████████████████████████▋                                       | 49/100 [2:13:13<1:11:07, 83.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20120728\n",
      "32\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|███████████████████████████████████████▌                                       | 50/100 [2:13:28<52:35, 63.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20151227\n",
      "200\n",
      "154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|██████████████████████████████████████▊                                     | 51/100 [2:19:50<2:09:48, 158.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20180210\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|███████████████████████████████████████▌                                    | 52/100 [2:19:52<1:29:17, 111.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20180219\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|████████████████████████████████████████▊                                    | 53/100 [2:19:53<1:01:28, 78.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20180220\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|██████████████████████████████████████████▋                                    | 54/100 [2:19:54<42:23, 55.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20180606\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|███████████████████████████████████████████▍                                   | 55/100 [2:19:55<29:17, 39.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allianz20160630\n",
      "405\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|██████████████████████████████████████████▌                                 | 56/100 [2:29:13<2:22:50, 194.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allinahealth20150901\n",
      "234\n",
      "204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|███████████████████████████████████████████▎                                | 57/100 [2:38:24<3:36:13, 301.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allonehealth20180919\n",
      "135\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|████████████████████████████████████████████                                | 58/100 [2:40:05<2:48:53, 241.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allonehealth20190203\n",
      "139\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|████████████████████████████████████████████▊                               | 59/100 [2:41:47<2:16:18, 199.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allpointseap20180919\n",
      "92\n",
      "92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████████████████████████████████████████████▌                              | 60/100 [2:43:29<1:53:37, 170.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almadinatakaful20160910\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████████████████████████████████████████████▎                             | 61/100 [2:43:31<1:17:47, 119.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almostfamily20151009\n",
      "32\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|████████████████████████████████████████████████▉                              | 62/100 [2:44:01<58:47, 92.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almostfamily20151209\n",
      "79\n",
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|█████████████████████████████████████████████████▊                             | 63/100 [2:45:04<51:41, 83.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpineinvestors20190228\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████████████████████████████████████████████████▌                            | 64/100 [2:45:04<35:21, 58.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amanat20170809\n",
      "264\n",
      "126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|███████████████████████████████████████████████████▎                           | 65/100 [2:47:57<54:11, 92.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "americaneldercare20150906\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|████████████████████████████████████████████████████▏                          | 66/100 [2:47:57<36:54, 65.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20140501\n",
      "279\n",
      "274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████▉                         | 67/100 [2:52:44<1:12:25, 131.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20141130\n",
      "195\n",
      "195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|███████████████████████████████████████████████████▋                        | 68/100 [2:55:35<1:16:33, 143.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20141225\n",
      "286\n",
      "277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|████████████████████████████████████████████████████▍                       | 69/100 [2:59:36<1:29:14, 172.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20190331\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|█████████████████████████████████████████████████████▏                      | 70/100 [2:59:39<1:00:59, 121.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerihealthcaritaspa20150501\n",
      "170\n",
      "153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|█████████████████████████████████████████████████████▉                      | 71/100 [3:02:57<1:09:56, 144.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerihealthcasualty20190301\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|████████████████████████████████████████████████████████▏                     | 72/100 [3:02:58<47:24, 101.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ameritas20180415\n",
      "122\n",
      "98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|████████████████████████████████████████████████████████▉                     | 73/100 [3:05:38<53:31, 118.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20120312\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|██████████████████████████████████████████████████████████▍                    | 74/100 [3:05:40<36:20, 83.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20130525\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████████████████████████████████████████████████████████▎                   | 75/100 [3:05:41<24:37, 59.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20141205\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|████████████████████████████████████████████████████████████                   | 76/100 [3:05:43<16:47, 41.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20150301\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|████████████████████████████████████████████████████████████▊                  | 77/100 [3:05:45<11:28, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20150509\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|█████████████████████████████████████████████████████████████▌                 | 78/100 [3:05:47<07:54, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20161003\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|██████████████████████████████████████████████████████████████▍                | 79/100 [3:05:48<05:27, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20180413\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|███████████████████████████████████████████████████████████████▏               | 80/100 [3:05:50<03:48, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amilassistenciamedica20170520\n",
      "37\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|███████████████████████████████████████████████████████████████▉               | 81/100 [3:06:56<08:48, 27.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amtrustfinancial20190301\n",
      "18\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████████████████████████████████████████████████████████████▊              | 82/100 [3:07:24<08:22, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amwestbrook20130401\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|█████████████████████████████████████████████████████████████████▌             | 83/100 [3:07:26<05:39, 19.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthem20170218\n",
      "226\n",
      "203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████▌            | 84/100 [3:12:38<28:42, 107.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthem20191221\n",
      "81\n",
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|██████████████████████████████████████████████████████████████████▎           | 85/100 [3:14:12<25:54, 103.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anvitahealth20131207\n",
      "48\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|███████████████████████████████████████████████████████████████████▉           | 86/100 [3:14:39<18:48, 80.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aonbenfield20161104\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████████████████████████████████████████████████████████████████▋          | 87/100 [3:14:40<12:18, 56.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apax20141002\n",
      "500\n",
      "127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████▋         | 88/100 [3:23:07<38:21, 191.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apax20151212\n",
      "383\n",
      "322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████▍        | 89/100 [3:29:54<46:57, 256.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apexmedicalcorp20171218\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████▏       | 90/100 [3:29:54<29:55, 179.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apgis20120924\n",
      "18\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████▉       | 91/100 [3:30:00<19:06, 127.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apollomunichinsurance20180127\n",
      "324\n",
      "223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████▊      | 92/100 [3:36:52<28:21, 212.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apshealthcare20140302\n",
      "66\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████▌     | 93/100 [3:37:37<18:56, 162.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argenta20120716\n",
      "310\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▎    | 94/100 [3:41:56<19:09, 191.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argolimited20161016\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 95/100 [3:41:58<11:13, 134.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascensioncaremanagement20170223\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████▊   | 96/100 [3:41:59<06:18, 94.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asiacapitalre20190202\n",
      "45\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████▋  | 97/100 [3:42:33<03:49, 76.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asisa20141228\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████████████████████████████████████████████████████████████████████████▍ | 98/100 [3:42:35<01:47, 53.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistanceonline-china20140430\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|██████████████████████████████████████████████████████████████████████████████▏| 99/100 [3:42:35<00:37, 37.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assor20121124\n",
      "212\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [3:45:28<00:00, 78.33s/it]\n"
     ]
    }
   ],
   "source": [
    "website_items_aft = dict()\n",
    "noLang = dict()\n",
    "forbidden_strings = ['personal information'',use', 'usage', 'disclaim', 'terms and conditions', 'terms & conditions', 'copyright', 'privacy policy', 'javascript', 'browser', 'loading']\n",
    "forbidden_strings2 = ['personal information','disclaim', 'terms and conditions', 'terms & conditions', 'copyright', 'privacy policy', 'javascript', 'browser', 'loading']\n",
    "for key in tqdm(soups_aft.keys()):\n",
    "    #companytext= str()\n",
    "    soup = soups_aft[key]\n",
    "   \n",
    "    items = soup.find_all('item')\n",
    "    print(key)\n",
    "    print(len(items))\n",
    "    item_list  = list()\n",
    "    \n",
    "    for item in items:\n",
    "        url = item.crawl_url.text\n",
    "        \n",
    "        if ('about' in url.lower()) and (re.search(r'\\b' + 'us' + r'\\b', url.lower())):\n",
    "            pass\n",
    "        elif ('contact' in url.lower()) and (re.search(r'\\b' + 'us' + r'\\b', url.lower())):\n",
    "            pass\n",
    "        elif ('privacy') in url.lower():\n",
    "            pass\n",
    "        elif ('career' in url.lower()):\n",
    "            pass\n",
    "        elif ('job' in url.lower()):\n",
    "            pass\n",
    "        elif ('vacanc' in url.lower()):\n",
    "            pass\n",
    "        elif ('terms' in url.lower()):\n",
    "            pass\n",
    "        elif ('news' in url.lower()):\n",
    "            pass\n",
    "        elif ('blog' in url.lower()):\n",
    "            pass\n",
    "        elif ('facebook' in url.lower()):\n",
    "            pass\n",
    "        elif ('twitter' in url.lower()):\n",
    "            pass\n",
    "        elif ('flickr' in url.lower()):\n",
    "            pass\n",
    "        elif ('youtube' in url.lower()):\n",
    "            pass\n",
    "        elif ('team' in url.lower()):\n",
    "            pass\n",
    "        if ('relations' in url.lower()):\n",
    "            pass\n",
    "        if ('director' in url.lower()):\n",
    "            pass\n",
    "        if ('executive' in url.lower()):\n",
    "            pass\n",
    "        if ('sitemap' in url.lower()):\n",
    "            pass\n",
    "        else:\n",
    "            it = dict()\n",
    "            if item.starturl is not None:\n",
    "                it['starturl'] = item.starturl.text\n",
    "            if item.started_url is not None:\n",
    "                it['started_url'] = item.started_url.text\n",
    "            if item.requested_date is not None:\n",
    "                it['requested_date']=item.requested_date.text\n",
    "            if item.start_date_max is not None:\n",
    "                it['start_date_max']=item.start_date_max.text\n",
    "            if item.crawl_url is not None:\n",
    "                it['crawl_url'] = item.crawl_url.text\n",
    "            if item.start_date is not None:\n",
    "                it['start_date']=item.start_date.text\n",
    "            if item.company is not None:\n",
    "                it['company']=item.company.text\n",
    "            if item.urlpartdate is not None:\n",
    "                it['urlpartdate']=item.urlpartdate.text\n",
    "\n",
    "            text = str(item.html)\n",
    "\n",
    "            text= BeautifulSoup(text, 'html')\n",
    "\n",
    "            html =text.string\n",
    "         \n",
    "            if html is not None:\n",
    "                if (html.startswith('!doctype')) or (html.startswith('!DOCTYPE')):\n",
    "                    html = \"<\" + html\n",
    "\n",
    "                if html.endswith('--'):\n",
    "                    html = html +'>'\n",
    "\n",
    "            if (html is not None):\n",
    "        #Do this for now, but check if wrong HTML (not recognized as HTML) can be fixed and how\n",
    "                if (bool(BeautifulSoup(html, \"html.parser\").find())==True):\n",
    "\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    \n",
    "                    regex = re.compile('.*menu.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*nav.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Menu.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Nav.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*head.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*foot.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*bar.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Bar.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*disclaim.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Disclaim.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*term.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*Term.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*condition.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*Condition.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*copy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Copy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*video.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Video.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*contact.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*Contact.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*comment.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Comment.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*address.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Address.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*cookie.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Cookie.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*privacy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Privacy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    for head in soup.find_all('head'):\n",
    "                        head.decompose()\n",
    "                    for header in soup.find_all('header'):\n",
    "                        header.decompose()\n",
    "                    for comment in soup.find_all('comment'):\n",
    "                        comment.decompose()\n",
    "\n",
    "                    for nav in soup.find_all('nav'):\n",
    "                        nav.decompose()\n",
    "\n",
    "                    for footer in soup.find_all('footer'):\n",
    "                        footer.decompose()\n",
    "\n",
    "                    for style in soup.find_all('style'):\n",
    "                        style.decompose()\n",
    "\n",
    "                    for script in soup.find_all('script'):\n",
    "                        script.decompose()\n",
    "\n",
    "                    for meta in soup.find_all('meta'):\n",
    "                        meta.decompose()\n",
    "                    \n",
    "                    for img in soup.find_all('img'):\n",
    "                        img.decompose()\n",
    "\n",
    "\n",
    "\n",
    "                    text = str()\n",
    "                   \n",
    "                    try:\n",
    "                        if (detect(soup.get_text(\" \", strip=True)) =='en'):\n",
    "                            inter = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                            for i in inter:\n",
    "                                t = i.get_text(' ', strip=True)\n",
    "                                if any(re.search('\\\\b'+f+'\\\\b', t, flags=re.IGNORECASE) for f in forbidden_strings):\n",
    "                                    if ('site' in t) or ('Site' in t) or ('SITE' in t):\n",
    "                                        if any(re.search('\\\\b'+f+'\\\\b', t, flags=re.IGNORECASE) for f in forbidden_strings2):\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            text += t +' '\n",
    "                                    else:\n",
    "                                        text += t +' '\n",
    "                                else:\n",
    "                                    text += t+' '\n",
    "                            \n",
    "                            while text.find('success fail')!= -1 and text.find('TIMESTAMPS')!=-1:\n",
    "                                start = text.find('success fail')\n",
    "                                end = text.find('TIMESTAMPS')\n",
    "                                text = text[0:start:]+text[end +10::]\n",
    "                            while text.find('COLLECTED BY') != -1 and text.find('TIMESTAMPS')!= -1:\n",
    "                                start = text.find('COLLECTED BY')\n",
    "                                end = text.find('TIMESTAMPS')\n",
    "                                text = text[0:start:] +text[end+10::]\n",
    "                            while text.find('succes fail') != -1 and text.find('capture')!= -1:\n",
    "                                start = text.find('success fail')\n",
    "                                end = text.find('capture')\n",
    "                                text = text[0:start:] +text[end+7::]\n",
    "                            while text.find('Our collection') != -1 and text.find('found at') != -1:\n",
    "                                start = text.find('Our collection')\n",
    "                                end = text.find('found at')\n",
    "                                text = text[0:start:]+text[end+8::]\n",
    "                            while text.find('Web wide crawl') != -1 and text.find('will be considered'):\n",
    "                                start = text.find('Web wide crawl')\n",
    "                                end = text.find('considered')\n",
    "                                text = text[0:start:] + text[end+10::]\n",
    "                            while text.find('The seed for this crawl') != -1 and text.find('general public'):\n",
    "                                start = text.find('The seed for this crawl')\n",
    "                                end = text.find('general public')\n",
    "                                text = text[0:start:] + text[end+14::]\n",
    "                            while text.find('History is littered') != -1 and text.find('ArchiveBots source code can be found at'):\n",
    "                                start = text.find('History is littered')\n",
    "                                end = text.find('found at')\n",
    "                                text = text[0:start:] + text[end+8::]\n",
    "                                \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            remove_digits = str.maketrans('', '', digits)\n",
    "                            text= text.translate(remove_digits)\n",
    "                            if len(text) != 0:\n",
    "                                try:\n",
    "\n",
    "                                    #Check if English and if no more html is in the text\n",
    "                                    if (detect(text) =='en') and (bool(BeautifulSoup(text, 'html.parser').find())==False):                \n",
    "                                        text = text.strip()\n",
    "                                        text = \" \".join(text.split())\n",
    "                                        #Remove special characters\n",
    "                                        text = re.sub('[^a-zA-Z.\\d\\s]', '', text)\n",
    "                                        text = nltk.word_tokenize(text)\n",
    "                                        text = [word for word in text if (word.find('.')==-1) or (word.find('.')==len(word)-1)]\n",
    "                                        text= TreebankWordDetokenizer().detokenize(text)\n",
    "                                        #s = SequenceMatcher(None, text, companytext)\n",
    "                                        #matches = s.get_matching_blocks()\n",
    "                          \n",
    "                                        #m = [match for match in matches if match[2]>50]\n",
    "                                       \n",
    "                                        #for match in m:\n",
    "                                        #    matched_part = companytext[match[1]:match[1]+match[2]]\n",
    "                                        #    print(matched_part)\n",
    "                                           \n",
    "                                        #    text = text.replace(matched_part, \"\")\n",
    "                                        #print(text)\n",
    "                                        it['html'] = text\n",
    "                                       \n",
    "                                       # companytext += text + ' '\n",
    "                                        \n",
    "                                        item_list.append(it)\n",
    "                                except:\n",
    "                                    noLang[key] = text\n",
    "                    except:\n",
    "                        noLang[key] = soup\n",
    "                            \n",
    "                  \n",
    "    #print(companytext)\n",
    "    print(len(item_list))\n",
    "   \n",
    "    website_items_aft[key] = item_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_website_scraper_aft = pd.DataFrame()\n",
    "for key, values in website_items_aft.items():\n",
    "    for item in website_items_aft[key]:\n",
    "        new_row = pd.DataFrame.from_dict([item])\n",
    "        new_website_scraper_aft=new_website_scraper_aft.append(new_row, ignore_index=True)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain sequences are repeated on every page of a website. To get rid of these double parts, all sequences longer than 50 characters that happen more than once for a company are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/82 [00:00<?, ?it/s]\n",
      "  1%|▉                                                                               | 1/82 [05:09<6:57:13, 309.05s/it]\n",
      "  6%|████▉                                                                           | 5/82 [05:28<4:39:29, 217.78s/it]\n",
      "  9%|██████▊                                                                         | 7/82 [05:29<3:10:43, 152.58s/it]\n",
      " 10%|███████▊                                                                        | 8/82 [05:34<2:13:33, 108.29s/it]\n",
      " 11%|████████▉                                                                        | 9/82 [05:59<1:41:24, 83.35s/it]\n",
      " 13%|██████████▋                                                                     | 11/82 [08:21<1:34:17, 79.69s/it]\n",
      " 17%|█████████████▋                                                                  | 14/82 [08:22<1:03:19, 55.88s/it]\n",
      " 18%|███████████████                                                                   | 15/82 [08:40<49:50, 44.63s/it]\n",
      " 20%|███████████████▌                                                                | 16/82 [10:14<1:05:10, 59.26s/it]\n",
      " 21%|████████████████▌                                                               | 17/82 [11:37<1:12:07, 66.58s/it]\n",
      " 22%|█████████████████▌                                                              | 18/82 [13:05<1:17:46, 72.91s/it]\n",
      " 23%|██████████████████▌                                                             | 19/82 [15:40<1:42:23, 97.52s/it]\n",
      " 24%|███████████████████▌                                                            | 20/82 [17:25<1:42:56, 99.62s/it]\n",
      " 26%|███████████████████▏                                                       | 21/82 [3:54:38<67:26:02, 3979.71s/it]\n",
      " 27%|████████████████████                                                       | 22/82 [3:57:39<47:20:01, 2840.02s/it]\n",
      " 28%|█████████████████████                                                      | 23/82 [4:29:02<41:50:29, 2553.05s/it]\n",
      " 29%|█████████████████████▉                                                     | 24/82 [4:44:04<33:09:01, 2057.60s/it]\n",
      " 30%|██████████████████████▊                                                    | 25/82 [5:33:37<36:55:35, 2332.19s/it]\n",
      " 34%|█████████████████████████▌                                                 | 28/82 [5:33:37<24:29:19, 1632.58s/it]\n",
      " 37%|███████████████████████████▍                                               | 30/82 [6:02:10<20:13:04, 1399.70s/it]\n",
      " 38%|████████████████████████████▋                                               | 31/82 [6:02:31<13:58:18, 986.25s/it]\n",
      " 39%|█████████████████████████████▋                                              | 32/82 [6:04:57<10:11:51, 734.24s/it]\n",
      " 40%|██████████████████████████████▏                                            | 33/82 [6:34:19<14:11:28, 1042.61s/it]\n",
      " 41%|███████████████████████████████▉                                             | 34/82 [6:34:35<9:47:31, 734.40s/it]\n",
      " 43%|████████████████████████████████▊                                            | 35/82 [6:42:41<8:36:54, 659.88s/it]\n",
      " 44%|█████████████████████████████████▎                                          | 36/82 [7:06:50<11:27:22, 896.58s/it]\n",
      " 45%|██████████████████████████████████▋                                          | 37/82 [7:06:52<7:51:19, 628.44s/it]\n",
      " 48%|████████████████████████████████████▌                                        | 39/82 [7:07:00<5:16:02, 440.98s/it]\n",
      " 49%|█████████████████████████████████████▌                                       | 40/82 [7:09:29<4:07:28, 353.55s/it]\n",
      " 50%|██████████████████████████████████████▌                                      | 41/82 [7:09:29<2:49:09, 247.56s/it]\n",
      " 51%|███████████████████████████████████████▍                                     | 42/82 [7:09:30<1:55:39, 173.50s/it]\n",
      " 52%|████████████████████████████████████████▍                                    | 43/82 [7:09:31<1:19:08, 121.77s/it]\n",
      " 54%|██████████████████████████████████████████▉                                     | 44/82 [7:09:32<54:04, 85.39s/it]\n",
      " 55%|███████████████████████████████████████████▉                                    | 45/82 [7:09:32<36:54, 59.85s/it]\n",
      " 56%|████████████████████████████████████████████▉                                   | 46/82 [7:10:06<31:16, 52.11s/it]\n",
      " 63%|██████████████████████████████████████████████████▋                             | 52/82 [7:13:35<23:27, 46.92s/it]\n",
      " 65%|███████████████████████████████████████████████████▋                            | 53/82 [7:13:55<18:47, 38.87s/it]\n",
      " 66%|████████████████████████████████████████████████████▋                           | 54/82 [7:14:17<15:51, 33.97s/it]\n",
      " 67%|█████████████████████████████████████████████████████▋                          | 55/82 [7:14:34<12:59, 28.86s/it]\n",
      " 70%|███████████████████████████████████████████████████████▌                        | 57/82 [7:14:36<08:32, 20.48s/it]\n",
      " 71%|████████████████████████████████████████████████████████▌                       | 58/82 [7:14:42<06:29, 16.25s/it]\n",
      " 73%|██████████████████████████████████████████████████████████▌                     | 60/82 [7:15:13<05:51, 15.98s/it]\n",
      " 74%|███████████████████████████████████████████████████████████▌                    | 61/82 [7:19:03<28:02, 80.10s/it]\n",
      " 76%|████████████████████████████████████████████████████████████▍                   | 62/82 [7:21:01<30:31, 91.58s/it]\n",
      " 77%|████████████████████████████████████████████████████████████▋                  | 63/82 [7:24:54<42:27, 134.07s/it]\n",
      " 78%|██████████████████████████████████████████████████████████████▍                 | 64/82 [7:24:55<28:12, 94.05s/it]\n",
      " 79%|███████████████████████████████████████████████████████████████▍                | 65/82 [7:25:43<22:45, 80.30s/it]\n",
      " 82%|█████████████████████████████████████████████████████████████████▎              | 67/82 [7:26:56<16:46, 67.07s/it]\n",
      " 83%|██████████████████████████████████████████████████████████████████▎             | 68/82 [7:26:59<11:09, 47.82s/it]\n",
      " 85%|████████████████████████████████████████████████████████████████████▎           | 70/82 [7:31:32<14:54, 74.56s/it]\n",
      " 87%|█████████████████████████████████████████████████████████████████████▎          | 71/82 [7:32:16<11:58, 65.30s/it]\n",
      " 88%|██████████████████████████████████████████████████████████████████████▏         | 72/82 [7:32:17<07:39, 45.95s/it]\n",
      " 89%|███████████████████████████████████████████████████████████████████████▏        | 73/82 [7:33:13<07:20, 48.96s/it]\n",
      " 90%|███████████████████████████████████████████████████████████████████████▎       | 74/82 [7:43:58<30:22, 227.86s/it]\n",
      " 91%|████████████████████████████████████████████████████████████████████████▎      | 75/82 [7:45:17<21:22, 183.23s/it]\n",
      " 93%|█████████████████████████████████████████████████████████████████████████▏     | 76/82 [7:45:30<13:13, 132.17s/it]\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████  | 80/82 [7:45:33<03:05, 92.75s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 82/82 [7:45:36<00:00, 65.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for instance in tqdm(new_website_scraper_aft['urlpartdate'].unique()):\n",
    "    texts = new_website_scraper_aft[new_website_scraper_aft['urlpartdate']==instance]['html']\n",
    "    indices = list(new_website_scraper_aft[new_website_scraper_aft['urlpartdate']==instance].index)\n",
    "    if len(texts)>1:\n",
    "        for i in indices[:-1]:\n",
    "            index1=i\n",
    "            textone = new_website_scraper_aft.iloc[index1]['html']\n",
    "            for j in indices[indices.index(i+1):]:\n",
    "                index2 = j\n",
    "                texttwo = new_website_scraper_aft.iloc[index2]['html']\n",
    "                s = SequenceMatcher(None,textone,texttwo)\n",
    "                matches = s.get_matching_blocks()\n",
    "                for match in matches:\n",
    "                    if match[2]>50:\n",
    "                        matched_part = textone[match[0]:match[0]+match[2]]\n",
    "                        new_website_scraper_aft.loc[index2,'html'] = texttwo.replace(matched_part,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the crawled URLs to a shorter form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new_website_scraper_aft.iterrows():\n",
    "    if new_website_scraper_aft.iloc[index]['crawl_url'][43:].startswith('http://'):\n",
    "        url = str(new_website_scraper_aft.iloc[index]['crawl_url'][43:].replace('http://', \"\"))\n",
    "    if new_website_scraper_aft.iloc[index]['crawl_url'][43:].startswith('https://'):\n",
    "        url = str(new_website_scraper_aft.iloc[index]['crawl_url'][43:].replace('https://', \"\"))\n",
    "    if url.startswith('www.'):\n",
    "        url = str(url.replace('www.',\"\"))\n",
    "    new_website_scraper_aft.at[index, 'crawl_url'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7702, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_website_scraper_aft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_website_scraper_aft.to_excel('D:/ProcessedData/after.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
