{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from string import digits\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from difflib import SequenceMatcher\n",
    "from random import sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the (new) data from the Scraper for before the date effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Scraper/BeforeProcessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups_bef = dict()\n",
    "companies_bef = os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the files of the scraped websites and create a BeautifulSoup object for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:05<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(companies_bef):\n",
    "    with open(file, encoding='utf-8') as data:\n",
    "        soup = BeautifulSoup(data, 'html.parser',from_encoding='utf-8')\n",
    "        soups_bef.update({file[:-4]:soup})\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the items for every website to a dictionary. Only process items that do not have a 'forbidden string' in their URL. Remove all style, script and metadata from the html. Only if the text stored between the <html> tags is recognized as HTML, the item is added to the dataset. Also remove the metadata from Wayback Machine by removing the banners and the text between \"success fail\" and \"TIMESTAMPS\". Remove all words that have a . in the middle of the word and remove all numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800contacts20120613\n",
      "76\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▊                                                                               | 1/100 [01:13<2:01:25, 73.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1hsa20100412\n",
      "4\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▌                                                                              | 2/100 [01:15<1:24:56, 52.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa20120524\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██▍                                                                               | 3/100 [01:16<59:17, 36.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa20130717\n",
      "295\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▏                                                                            | 4/100 [04:35<2:16:57, 85.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abaseguros20130425\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████                                                                            | 5/100 [04:36<1:35:14, 60.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcassicura20100723\n",
      "28\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|████▊                                                                           | 6/100 [04:48<1:11:21, 45.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdainsurance20131009\n",
      "141\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                          | 7/100 [06:09<1:27:11, 56.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aberdare20140310\n",
      "73\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▍                                                                         | 8/100 [06:48<1:18:10, 50.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abrihealthplan20100825\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████▍                                                                          | 9/100 [06:48<54:20, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolutetotalcare20100525\n",
      "214\n",
      "165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███████▉                                                                       | 10/100 [09:29<1:50:00, 73.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acariahealth20130325\n",
      "34\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████████▋                                                                      | 11/100 [09:55<1:27:57, 59.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessdental20171026\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▍                                                                     | 12/100 [09:57<1:01:30, 41.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessintegra20150826\n",
      "253\n",
      "166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████▎                                                                    | 13/100 [13:05<2:04:30, 85.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessplansusa20120524\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████                                                                    | 14/100 [13:07<1:26:50, 60.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accredited-inc20141120\n",
      "4\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|███████████▊                                                                   | 15/100 [13:10<1:01:19, 43.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acibademsigorta20131106\n",
      "119\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|████████████▋                                                                  | 16/100 [14:49<1:24:17, 60.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acrcapitalholdings20120508\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████▊                                                                   | 17/100 [14:50<58:39, 42.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20110621\n",
      "338\n",
      "285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████████████                                                                | 18/100 [19:38<2:38:36, 116.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20110923\n",
      "314\n",
      "271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|██████████████▊                                                               | 19/100 [25:06<4:02:23, 179.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20110924\n",
      "253\n",
      "206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████▌                                                              | 20/100 [29:32<4:33:58, 205.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20110928\n",
      "206\n",
      "198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████████████▍                                                             | 21/100 [35:42<5:35:39, 254.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20130325\n",
      "292\n",
      "273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████████████▏                                                            | 22/100 [43:13<6:47:43, 313.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20130430\n",
      "264\n",
      "254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████████████▉                                                            | 23/100 [49:31<7:07:24, 333.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20140416\n",
      "298\n",
      "290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████████████████▋                                                           | 24/100 [58:05<8:10:31, 387.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20141118\n",
      "295\n",
      "292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████████████                                                         | 25/100 [1:05:23<8:23:16, 402.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetna20170718\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████████████▊                                                        | 26/100 [1:05:24<5:47:49, 282.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afaforsakring20160325\n",
      "57\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|████████████████████▌                                                       | 27/100 [1:06:00<4:13:23, 208.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affinityinsuranceservices20120524\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|█████████████████████▎                                                      | 28/100 [1:06:01<2:55:12, 146.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agateresources20150923\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████████████████                                                      | 29/100 [1:06:01<2:01:02, 102.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmchealth20111111\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████████████████                                                      | 30/100 [1:06:03<1:24:15, 72.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmpr20110201\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████████████████▍                                                      | 31/100 [1:06:04<58:23, 50.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahn20130422\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████████████████████▎                                                     | 32/100 [1:06:06<41:02, 36.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahni20170515\n",
      "500\n",
      "488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████████████████                                                   | 33/100 [1:11:36<2:18:49, 124.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aia20101229\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████████████████▏                                                  | 34/100 [1:11:37<1:36:00, 87.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aig20170503\n",
      "500\n",
      "476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████████████████▌                                                 | 35/100 [1:19:57<3:48:35, 211.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aipinsurance20110715\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████████████████▎                                                | 36/100 [1:19:57<2:37:42, 147.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajg20151201\n",
      "500\n",
      "461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|████████████████████████████                                                | 37/100 [1:29:34<4:50:24, 276.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajg20161214\n",
      "500\n",
      "470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|████████████████████████████▉                                               | 38/100 [1:39:48<6:30:18, 377.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alamoinsurance20091229\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|█████████████████████████████▋                                              | 39/100 [1:39:49<4:29:01, 264.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alan20161018\n",
      "500\n",
      "491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|██████████████████████████████▍                                             | 40/100 [1:51:26<6:34:28, 394.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alchealth20170223\n",
      "98\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████████████████████▏                                            | 41/100 [1:53:02<4:59:53, 304.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegeus20130417\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████████████████████▉                                            | 42/100 [1:53:03<3:26:30, 213.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20110711\n",
      "50\n",
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████████████████████▋                                           | 43/100 [1:54:12<2:41:53, 170.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20120424\n",
      "125\n",
      "103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████████████████████▍                                          | 44/100 [1:57:02<2:38:44, 170.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20130927\n",
      "33\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████████████████████▏                                         | 45/100 [1:57:32<1:57:24, 128.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignnetworks20131205\n",
      "32\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████████████████████▍                                         | 46/100 [1:58:01<1:28:42, 98.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20100721\n",
      "26\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████████████████████▏                                        | 47/100 [1:58:53<1:14:42, 84.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20131220\n",
      "22\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████████████████████████████▉                                         | 48/100 [1:59:08<55:02, 63.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20160203\n",
      "180\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████████████████████▏                                      | 49/100 [2:06:10<2:25:28, 171.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20160212\n",
      "180\n",
      "142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████████████████████████                                      | 50/100 [2:12:24<3:13:29, 232.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20160213\n",
      "176\n",
      "139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|██████████████████████████████████████▊                                     | 51/100 [2:17:36<3:29:06, 256.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleghany20160530\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████████████████████████▌                                    | 52/100 [2:17:37<2:23:40, 179.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allianz20140623\n",
      "500\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████████████████████████▎                                   | 53/100 [2:27:32<3:58:12, 304.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allinahealth20130825\n",
      "427\n",
      "343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████████████████████████                                   | 54/100 [2:38:12<5:10:28, 404.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allonehealth20160912\n",
      "500\n",
      "492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████████████████████████▊                                  | 55/100 [2:48:02<5:45:18, 460.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allonehealth20170127\n",
      "500\n",
      "487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|██████████████████████████████████████████▌                                 | 56/100 [2:56:29<5:47:57, 474.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allpointseap20160912\n",
      "68\n",
      "68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████████████████████████████████▎                                | 57/100 [2:57:41<4:13:26, 353.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almadinatakaful20140903\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████████████████████████                                | 58/100 [2:57:43<2:53:35, 247.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almostfamily20131002\n",
      "41\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████████████████████████▊                               | 59/100 [2:58:08<2:03:46, 181.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almostfamily20131202\n",
      "15\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████▌                              | 60/100 [2:58:17<1:26:23, 129.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amanat20150802\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|████████████████████████████████████████████████▏                              | 61/100 [2:58:18<59:06, 90.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20120424\n",
      "96\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████████████████████████████▉                              | 62/100 [2:59:36<55:11, 87.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20121123\n",
      "192\n",
      "177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████████████████████████████▉                            | 63/100 [3:01:58<1:03:47, 103.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20121218\n",
      "209\n",
      "204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████████████████████████████▋                           | 64/100 [3:04:44<1:13:25, 122.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerigroup20170324\n",
      "7\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████████████████████████████▎                           | 65/100 [3:04:50<51:04, 87.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerihealthcasualty20170222\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████████████████████████████████▏                          | 66/100 [3:04:52<35:04, 61.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amerihealthmercy20111124\n",
      "311\n",
      "290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████████████████████████████▉                         | 67/100 [3:10:29<1:19:19, 144.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ameritas20160408\n",
      "39\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████████████████████████████████████▋                        | 68/100 [3:11:19<1:01:56, 116.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20100305\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████████████████████████████████▌                        | 69/100 [3:11:20<42:10, 81.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20110518\n",
      "69\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████████████████████████████████▎                       | 70/100 [3:12:18<37:08, 74.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20121128\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|████████████████████████████████████████████████████████                       | 71/100 [3:12:20<25:30, 52.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20130222\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████████████████████████████████▉                      | 72/100 [3:12:23<17:36, 37.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20130502\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|█████████████████████████████████████████████████████████▋                     | 73/100 [3:12:24<12:04, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20140926\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|██████████████████████████████████████████████████████████▍                    | 74/100 [3:12:26<08:21, 19.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amil20160406\n",
      "220\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████████████████████████████████▎                   | 75/100 [3:16:49<38:34, 92.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amilassistenciamedica20150513\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████████████████████████████████                   | 76/100 [3:16:51<26:04, 65.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amwestbrook20110325\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|████████████████████████████████████████████████████████████▊                  | 77/100 [3:16:53<17:43, 46.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthem20150211\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████████████████████████████████▌                 | 78/100 [3:16:55<12:05, 32.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthem20171214\n",
      "500\n",
      "417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|████████████████████████████████████████████████████████████                | 79/100 [3:26:52<1:10:44, 202.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anvitahealth20111130\n",
      "115\n",
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████████████████████████████████▍               | 80/100 [3:27:50<53:00, 159.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aonbenfield20141028\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████████████████████████████████████▏              | 81/100 [3:27:51<35:18, 111.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apax20120925\n",
      "500\n",
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████████████████████████████████████▎             | 82/100 [3:35:27<1:04:30, 215.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apax20131205\n",
      "413\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████████████████████████████████████             | 83/100 [3:42:01<1:16:08, 268.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apexmedicalcorp20151211\n",
      "4\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████▌            | 84/100 [3:42:03<50:16, 188.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apgis20100917\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|██████████████████████████████████████████████████████████████████▎           | 85/100 [3:42:05<33:09, 132.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apollomunichinsurance20160120\n",
      "275\n",
      "164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████████           | 86/100 [3:46:38<40:45, 174.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apshealthcare20120224\n",
      "184\n",
      "163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████▊          | 87/100 [3:48:26<33:30, 154.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argenta20100709\n",
      "500\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████▋         | 88/100 [3:52:38<36:48, 184.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asiacapitalre20170126\n",
      "288\n",
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████▍        | 89/100 [3:55:34<33:17, 181.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asisa20121221\n",
      "291\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████▏       | 90/100 [3:59:32<33:06, 198.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistanceonline-china20120423\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████▉       | 91/100 [3:59:34<20:54, 139.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assor20101117\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████████▋      | 92/100 [3:59:35<13:02, 97.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assuria20150224\n",
      "272\n",
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████▌     | 93/100 [4:02:25<13:56, 119.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astrazenecavoluntarybenefits20170623\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|██████████████████████████████████████████████████████████████████████████▎    | 94/100 [4:02:25<08:22, 83.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlasconsulting20150807\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████████████████████████████████████████████    | 95/100 [4:02:26<04:55, 59.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avalere20150825\n",
      "483\n",
      "482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▉   | 96/100 [4:15:07<17:57, 269.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant20120330\n",
      "87\n",
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▋  | 97/100 [4:16:03<10:16, 205.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averoachmea20101230\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▍ | 98/100 [4:16:04<04:48, 144.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aviabel20160923\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▏| 99/100 [4:16:05<01:41, 101.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aviabel20170325\n",
      "269\n",
      "103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [4:19:33<00:00, 133.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "website_items_bef = dict()\n",
    "noLang = dict()\n",
    "forbidden_strings = ['personal information'',use', 'usage', 'disclaim', 'terms and conditions', 'terms & conditions', 'copyright', 'privacy policy', 'javascript', 'browser', 'loading']\n",
    "forbidden_strings2 = ['personal information','disclaim', 'terms and conditions', 'terms & conditions', 'copyright', 'privacy policy', 'javascript', 'browser', 'loading']\n",
    "for key in tqdm(soups_bef.keys()):\n",
    "    soup = soups_bef[key]\n",
    "   \n",
    "    items = soup.find_all('item')\n",
    "    print(key)\n",
    "    print(len(items))\n",
    "    item_list  = list()\n",
    "    \n",
    "    for item in items:\n",
    "        url = item.crawl_url.text\n",
    "        \n",
    "        if ('about' in url.lower()) and (re.search(r'\\b' + 'us' + r'\\b', url.lower())):\n",
    "            pass\n",
    "        elif ('contact' in url.lower()) and (re.search(r'\\b' + 'us' + r'\\b', url.lower())):\n",
    "            pass\n",
    "        elif ('privacy') in url.lower():\n",
    "            pass\n",
    "        elif ('career' in url.lower()):\n",
    "            pass\n",
    "        elif ('job' in url.lower()):\n",
    "            pass\n",
    "        elif ('vacanc' in url.lower()):\n",
    "            pass\n",
    "        elif ('terms' in url.lower()):\n",
    "            pass\n",
    "        elif ('news' in url.lower()):\n",
    "            pass\n",
    "        elif ('blog' in url.lower()):\n",
    "            pass\n",
    "        elif ('facebook' in url.lower()):\n",
    "            pass\n",
    "        elif ('twitter' in url.lower()):\n",
    "            pass\n",
    "        elif ('flickr' in url.lower()):\n",
    "            pass\n",
    "        elif ('youtube' in url.lower()):\n",
    "            pass\n",
    "        elif ('team' in url.lower()):\n",
    "            pass\n",
    "        if ('relations' in url.lower()):\n",
    "            pass\n",
    "        if ('director' in url.lower()):\n",
    "            pass\n",
    "        if ('executive' in url.lower()):\n",
    "            pass\n",
    "        if ('sitemap' in url.lower()):\n",
    "            pass\n",
    "        else:\n",
    "            it = dict()\n",
    "            if item.starturl is not None:\n",
    "                it['starturl'] = item.starturl.text\n",
    "            if item.started_url is not None:\n",
    "                it['started_url'] = item.started_url.text\n",
    "            if item.requested_date is not None:\n",
    "                it['requested_date']=item.requested_date.text\n",
    "            if item.start_date_max is not None:\n",
    "                it['start_date_max']=item.start_date_max.text\n",
    "            if item.crawl_url is not None:\n",
    "                it['crawl_url'] = item.crawl_url.text\n",
    "            if item.start_date is not None:\n",
    "                it['start_date']=item.start_date.text\n",
    "            if item.company is not None:\n",
    "                it['company']=item.company.text\n",
    "            if item.urlpartdate is not None:\n",
    "                it['urlpartdate']=item.urlpartdate.text\n",
    "\n",
    "            text = str(item.html)\n",
    "\n",
    "            text= BeautifulSoup(text, 'html')\n",
    "\n",
    "            html =text.string\n",
    "         \n",
    "            if html is not None:\n",
    "                if (html.startswith('!doctype')) or (html.startswith('!DOCTYPE')):\n",
    "                    html = \"<\" + html\n",
    "\n",
    "                if html.endswith('--'):\n",
    "                    html = html +'>'\n",
    "\n",
    "            if (html is not None):\n",
    "        #Do this for now, but check if wrong HTML (not recognized as HTML) can be fixed and how\n",
    "                if (bool(BeautifulSoup(html, \"html.parser\").find())==True):\n",
    "\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    \n",
    "                    regex = re.compile('.*menu.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*nav.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Menu.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Nav.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*head.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*foot.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*bar.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Bar.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*disclaim.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Disclaim.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*term.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*Term.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*condition.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*Condition.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*copy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Copy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*video.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Video.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*contact.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "                    regex = re.compile('.*Contact.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*comment.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Comment.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*address.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Address.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*cookie.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Cookie.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*privacy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    regex = re.compile('.*Privacy.*')\n",
    "                    for div in soup.find_all('div', {'class': regex}):\n",
    "                        div.decompose()\n",
    "                    for div in soup.find_all('div', {'id': regex}):\n",
    "                        div.decompose()\n",
    "                    for li in soup.find_all('li', {'class': regex}):\n",
    "                        li.decompose()\n",
    "                    for li in soup.find_all('li', {'id': regex}):\n",
    "                        li.decompose()\n",
    "                    for p in soup.find_all('p', {'class': regex}):\n",
    "                        p.decompose()\n",
    "                    for p in soup.find_all('p', {'id': regex}):\n",
    "                        p.decompose()\n",
    "                    for span in soup.find_all('span', {'class':regex}):\n",
    "                        span.decompose()\n",
    "                    for span in soup.find_all('span', {'id':regex}):\n",
    "                        span.decompose()\n",
    "                    for a in soup.find_all('a', {'href':regex}):\n",
    "                        a.decompose()\n",
    "\n",
    "\n",
    "                    for head in soup.find_all('head'):\n",
    "                        head.decompose()\n",
    "                    for header in soup.find_all('header'):\n",
    "                        header.decompose()\n",
    "                    for comment in soup.find_all('comment'):\n",
    "                        comment.decompose()\n",
    "\n",
    "                    for nav in soup.find_all('nav'):\n",
    "                        nav.decompose()\n",
    "\n",
    "                    for footer in soup.find_all('footer'):\n",
    "                        footer.decompose()\n",
    "\n",
    "                    for style in soup.find_all('style'):\n",
    "                        style.decompose()\n",
    "\n",
    "                    for script in soup.find_all('script'):\n",
    "                        script.decompose()\n",
    "\n",
    "                    for meta in soup.find_all('meta'):\n",
    "                        meta.decompose()\n",
    "                    \n",
    "                    for img in soup.find_all('img'):\n",
    "                        img.decompose()\n",
    "\n",
    "\n",
    "\n",
    "                    text = str()\n",
    "                   \n",
    "                    try:\n",
    "                        if (detect(soup.get_text(\" \", strip=True)) =='en'):\n",
    "                            inter = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                            for i in inter:\n",
    "                                t = i.get_text(' ', strip=True)\n",
    "                                if any(re.search('\\\\b'+f+'\\\\b', t, flags=re.IGNORECASE) for f in forbidden_strings):\n",
    "                                    if ('site' in t) or ('Site' in t) or ('SITE' in t):\n",
    "                                        if any(re.search('\\\\b'+f+'\\\\b', t, flags=re.IGNORECASE) for f in forbidden_strings2):\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            text += t +' '\n",
    "                                    else:\n",
    "                                        text += t +' '\n",
    "                                else:\n",
    "                                    text += t+' '\n",
    "                            \n",
    "                            while text.find('success fail')!= -1 and text.find('TIMESTAMPS')!=-1:\n",
    "                                start = text.find('success fail')\n",
    "                                end = text.find('TIMESTAMPS')\n",
    "                                text = text[0:start:]+text[end +10::]\n",
    "                            while text.find('COLLECTED BY') != -1 and text.find('TIMESTAMPS')!= -1:\n",
    "                                start = text.find('COLLECTED BY')\n",
    "                                end = text.find('TIMESTAMPS')\n",
    "                                text = text[0:start:] +text[end+10::]\n",
    "                            while text.find('succes fail') != -1 and text.find('capture')!= -1:\n",
    "                                start = text.find('success fail')\n",
    "                                end = text.find('capture')\n",
    "                                text = text[0:start:] +text[end+7::]\n",
    "                            while text.find('Our collection') != -1 and text.find('found at') != -1:\n",
    "                                start = text.find('Our collection')\n",
    "                                end = text.find('found at')\n",
    "                                text = text[0:start:]+text[end+8::]\n",
    "                            while text.find('Web wide crawl') != -1 and text.find('will be considered'):\n",
    "                                start = text.find('Web wide crawl')\n",
    "                                end = text.find('considered')\n",
    "                                text = text[0:start:] + text[end+10::]\n",
    "                            while text.find('The seed for this crawl') != -1 and text.find('general public'):\n",
    "                                start = text.find('The seed for this crawl')\n",
    "                                end = text.find('general public')\n",
    "                                text = text[0:start:] + text[end+14::]\n",
    "                            while text.find('History is littered') != -1 and text.find('ArchiveBots source code can be found at'):\n",
    "                                start = text.find('History is littered')\n",
    "                                end = text.find('found at')\n",
    "                                text = text[0:start:] + text[end+8::]\n",
    "                                \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            remove_digits = str.maketrans('', '', digits)\n",
    "                            text= text.translate(remove_digits)\n",
    "                            if len(text) != 0:\n",
    "                                try:\n",
    "\n",
    "                                    #Check if English and if no more html is in the text\n",
    "                                    if (detect(text) =='en') and (bool(BeautifulSoup(text, 'html.parser').find())==False):                \n",
    "                                        text = text.strip()\n",
    "                                        text = \" \".join(text.split())\n",
    "                                        #Remove special characters\n",
    "                                        text = re.sub('[^a-zA-Z.\\d\\s]', '', text)\n",
    "                                        text = nltk.word_tokenize(text)\n",
    "                                        text = [word for word in text if (word.find('.')==-1) or (word.find('.')==len(word)-1)]\n",
    "                                        text= TreebankWordDetokenizer().detokenize(text)\n",
    "                                        it['html'] = text\n",
    "                                        item_list.append(it)\n",
    "                                except:\n",
    "                                    noLang[key] = text\n",
    "                    except:\n",
    "                        noLang[key] = soup\n",
    "                            \n",
    "                  \n",
    "    #print(companytext)\n",
    "    print(len(item_list))\n",
    "   \n",
    "    website_items_bef[key] = item_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_website_scraper_bef = pd.DataFrame()\n",
    "for key, values in website_items_bef.items():\n",
    "    for item in website_items_bef[key]:\n",
    "        new_row = pd.DataFrame.from_dict([item])\n",
    "        new_website_scraper_bef=new_website_scraper_bef.append(new_row, ignore_index=True)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain sequences are repeated on every page of a website. To get rid of these double parts, all sequences longer than 50 characters that happen more than once for a company are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/70 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|█▏                                                                                 | 1/70 [00:23<27:32, 23.95s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|█████▉                                                                             | 5/70 [00:46<19:58, 18.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████████▎                                                                          | 7/70 [01:21<19:05, 18.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█████████▍                                                                         | 8/70 [01:22<13:37, 13.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████▋                                                                      | 10/70 [03:11<25:34, 25.57s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|██████████████▋                                                                | 13/70 [17:00<1:35:43, 100.76s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████▊                                                               | 14/70 [30:56<4:59:56, 321.36s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████████████▉                                                              | 15/70 [40:05<5:57:07, 389.59s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██████████████████                                                             | 16/70 [48:22<6:19:35, 421.77s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|███████████████████▏                                                           | 17/70 [57:11<6:40:57, 453.91s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████████████▊                                                         | 18/70 [1:06:47<7:05:12, 490.62s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|████████████████████▉                                                        | 19/70 [1:14:36<6:51:36, 484.24s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████████████████                                                       | 20/70 [1:26:38<7:42:51, 555.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████████████████▎                                                   | 23/70 [1:26:38<5:04:34, 388.81s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████████████████▍                                                  | 24/70 [1:37:32<5:58:59, 468.26s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|████████████████████████████▌                                                | 26/70 [2:03:59<6:55:02, 565.96s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|████████████████████████████▉                                              | 27/70 [2:37:37<11:57:41, 1001.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|██████████████████████████████                                             | 28/70 [3:13:56<15:48:15, 1354.64s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████████████████████                                            | 29/70 [3:24:10<12:53:54, 1132.55s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|█████████████████████████████████                                            | 30/70 [3:24:21<8:50:46, 796.17s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████████████████████                                           | 31/70 [3:24:24<6:02:48, 558.16s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████████████████████▏                                         | 32/70 [3:24:33<4:09:10, 393.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████████████████████▎                                        | 33/70 [3:24:34<2:49:55, 275.54s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████████████████████▍                                       | 34/70 [3:24:34<1:55:48, 193.02s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████████████████████████▌                                      | 35/70 [3:25:04<1:24:07, 144.22s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|████████████████████████████████████████▋                                      | 36/70 [3:25:05<57:14, 101.02s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████████████████████████▎                                     | 37/70 [3:25:57<47:34, 86.49s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|███████████████████████████████████████████▍                                    | 38/70 [3:26:46<40:05, 75.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|████████████████████████████████████████████▌                                   | 39/70 [3:27:43<35:56, 69.57s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████████████████████████████████████████████                                | 41/70 [4:09:49<3:26:43, 427.71s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████████████████████████████▏                              | 42/70 [4:33:54<5:41:59, 732.84s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████████████████████████████▎                             | 43/70 [4:58:00<7:05:59, 946.65s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████████████████████████████▍                            | 44/70 [4:58:17<4:49:20, 667.72s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████████████████████████████▌                           | 45/70 [4:58:20<3:15:10, 468.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████████████████████████████████████▌                          | 46/70 [4:58:21<2:11:14, 328.12s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|███████████████████████████████████████████████████▋                         | 47/70 [4:59:08<1:33:29, 243.91s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████████████████████████████▊                        | 48/70 [5:01:23<1:17:25, 211.14s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████████████████████████████▉                       | 49/70 [5:08:54<1:39:04, 283.05s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████████████████████████████                      | 50/70 [5:08:55<1:06:09, 198.47s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|██████████████████████████████████████████████████████████▋                    | 52/70 [5:14:37<57:04, 190.27s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████████████████████████████████████████████████████████▊                   | 53/70 [5:14:39<37:55, 133.85s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████████████████████████████████████████████████████████████▎              | 57/70 [5:48:35<53:23, 246.39s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|█████████████████████████████████████████████████████████████████▍             | 58/70 [5:48:37<34:35, 172.97s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████████████████████████████████▉            | 59/70 [6:15:50<1:51:59, 610.91s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|██████████████████████████████████████████████████████████████████           | 60/70 [6:40:02<2:23:55, 863.51s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████          | 61/70 [6:41:34<1:34:46, 631.79s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████████████████████████████████████████████████████████████████▏        | 62/70 [6:42:30<1:01:13, 459.15s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████        | 63/70 [6:47:38<48:16, 413.85s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████▎     | 65/70 [6:48:06<24:29, 293.89s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████████████████████████████████████████▋   | 67/70 [9:07:34<1:13:02, 1460.95s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▊  | 68/70 [9:07:47<34:12, 1026.45s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 70/70 [9:08:03<00:00, 721.02s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instance in tqdm(new_website_scraper_bef['urlpartdate'].unique()):\n",
    "    texts = new_website_scraper_bef[new_website_scraper_bef['urlpartdate']==instance]['html']\n",
    "    indices = list(new_website_scraper_bef[new_website_scraper_bef['urlpartdate']==instance].index)\n",
    "    if len(texts)>1:\n",
    "        for i in indices[:-1]:\n",
    "            index1=i\n",
    "            textone = new_website_scraper_bef.iloc[index1]['html']\n",
    "            for j in indices[indices.index(i+1):]:\n",
    "                index2 = j\n",
    "                texttwo = new_website_scraper_bef.iloc[index2]['html']\n",
    "                s = SequenceMatcher(None,textone,texttwo)\n",
    "                matches = s.get_matching_blocks()\n",
    "                for match in matches:\n",
    "                    if match[2]>50:\n",
    "                        matched_part = textone[match[0]:match[0]+match[2]]\n",
    "                        new_website_scraper_bef.loc[index2,'html'] = texttwo.replace(matched_part,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the crawled URLs to a shorter form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new_website_scraper_bef.iterrows():\n",
    "    if new_website_scraper_bef.iloc[index]['crawl_url'][43:].startswith('http://'):\n",
    "        url = str(new_website_scraper_bef.iloc[index]['crawl_url'][43:].replace('http://', \"\"))\n",
    "    if new_website_scraper_bef.iloc[index]['crawl_url'][43:].startswith('https://'):\n",
    "        url = str(new_website_scraper_bef.iloc[index]['crawl_url'][43:].replace('https://', \"\"))\n",
    "    if url.startswith('www.'):\n",
    "        url = str(url.replace('www.',\"\"))\n",
    "    new_website_scraper_bef.at[index, 'crawl_url'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10598, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_website_scraper_bef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_website_scraper_bef.to_excel('D:/ProcessedData/before.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
